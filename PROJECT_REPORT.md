# ProTrek Hard-Negative Fine-tuning Report


**Project:** Improving ProTrek (35M) via hard-negative mining + triplet fine-tuning  

**Students:** Wenyu Li, Zhangchi Huang, Haotian Shen

**Date:** 2026-01-09

## 1. Overview

This project starts from the **ProTrek-35M** baseline model and improves its ability to **separate positive protein–text pairs from confusing “twin-like” hard negatives**. The key idea is:

1. **Mine hard negatives**: proteins with **high sequence similarity** but **low text similarity** (functionally different descriptions).
2. **Fine-tune the baseline** using a **triplet margin loss** so that, for each text query, the embedding of its paired protein (anchor) scores higher than the hard-negative protein.

The report includes the required experimental screenshots/figures and a metric table comparing **baseline vs. fine-tuned** models.

## 2. Baseline & Problem Setting

### 2.1 Baseline model
- Baseline checkpoint: `weights/ProTrek_35M/ProTrek_35M.pt`
- Model family: ProTrek trimodal model (sequence, structure, and text encoders)

### 2.2 Task definition
For each mined sample we form a triplet:
- **Anchor protein sequence**: `anchor_seq`
- **Anchor text description**: `anchor_text`
- **Hard negative protein sequence**: `hard_neg_seq`

We compute similarity scores using dot product (as implemented in evaluation code):
- Positive score: $s_{pos} = \langle f_{prot}(anchor\_seq), f_{text}(anchor\_text) \rangle$
- Negative score: $s_{neg} = \langle f_{prot}(hard\_neg\_seq), f_{text}(anchor\_text) \rangle$

Goal: make $s_{pos}$ larger than $s_{neg}$ especially for **hard negative** cases.



## 3. Method

### 3.1 Hard negative mining
Hard negatives are mined from `protrek_data.tsv` by:
1. Grouping examples by `foldseek_seq` (structure-based cluster proxy).
2. Within each cluster, selecting protein pairs where:
   - **Text similarity** (TF-IDF cosine similarity) is below a threshold:
     - `TEXT_SIM_THRESHOLD = 0.5`
   - **Sequence similarity** (SequenceMatcher ratio) is high but not identical:
     - `MIN_SEQ_SIMILARITY = 0.7`
     - `MAX_SEQ_SIMILARITY = 0.9999`

This yields “twin proteins”: almost identical sequences but substantially different descriptions.

**Figure 1 (Hard negative example mined from the dataset).**

![](evaluation_results/hard_negative_examples.png)

Interpretation:
- The visualization highlights a high-identity pair and pinpoints a **single-point mutation**.
- The text descriptions differ, demonstrating why these pairs are hard: small sequence changes can correspond to large functional changes.

### 3.2 Fine-tuning objective (Triplet margin loss)
For each triplet (anchor protein, text, hard negative protein), we optimize:

$$
\mathcal{L} = \max(0,\; s_{neg} - s_{pos} + m)
$$

where the margin is:
- `MARGIN = 0.2`

This directly enforces the desired ordering $s_{pos} \ge s_{neg} + m$.

## 4. Experimental Setup

### 4.1 Data splits and sizes
Data is created and split by the pipeline:
- Hard negatives are saved as:
  - Train: `hard_negatives_train.csv`
  - Test: `hard_negatives_test.csv`
- Easy samples are saved as:
  - `easy_samples_all.csv`

Observed dataset sizes in this repo:

| Dataset | File | #Rows |
|---|---|---:|
| Hard negatives (train) | `hard_negatives_train.csv` | 545 |
| Hard negatives (test) | `hard_negatives_test.csv` | 137 |
| Easy samples (all) | `easy_samples_all.csv` | 1000 |

### 4.2 Training configuration
- Batch size: `24`
- Epochs: `30`
- Learning rate: `1e-5`
- Optimizer: AdamW, weight decay `1e-4`
- Device: `cuda` (auto-detected)
- Checkpointing: save every 5 epochs; final used checkpoint:
  - `weights/ProTrek_35M/protrek_finetuned_epoch30.pt`

### 4.3 Evaluation protocol
We evaluate on two datasets:
- **Hard samples**: the difficult mined pairs.
- **Easy samples**: randomly sampled non-hard examples for sanity/generalization.

The evaluation figures are generated by `evaluate.py` and stored in `evaluation_results/`.

## 5. Metrics

The assignment requires **precision and recall** comparisons. Since the model outputs a real-valued similarity score, we cast evaluation into two complementary metrics:

### 5.1 Pairwise ranking accuracy (used in the code)
This is the metric used in the provided evaluation script:

$$
\text{PairwiseAcc} = \Pr(s_{pos} > s_{neg})
$$

It measures how often the positive pair outranks its matched hard negative.

### 5.2 Precision / Recall (binary classification over scores)
We treat every score as a labeled example:
- Label 1: positive score $s_{pos}$
- Label 0: negative score $s_{neg}$

For a threshold $\tau$:
- Predict positive if $s \ge \tau$

We report:
- Precision = $\frac{TP}{TP+FP}$
- Recall = $\frac{TP}{TP+FN}$

Threshold selection (for reporting): we sweep thresholds over all unique scores in the dataset and pick the threshold that maximizes **F1** on that dataset. 

## 6. Results: Baseline vs. Fine-tuned

### 6.1 Quantitative comparison (Precision / Recall)

| Dataset | Model | PairwiseAcc ↑ | Precision ↑ | Recall ↑ | F1 ↑ | Best threshold $\tau$ |
|---|---|---:|---:|---:|---:|---:|
| Hard (n=137) | Baseline | 0.5620 | 0.5018 | 1.0000 | 0.6683 | 0.1963 |
| Hard (n=137) | Fine-tuned | **0.8978** | **0.7643** | 0.7810 | **0.7726** | 0.1402 |
| Easy (n=1000) | Baseline | **0.9810** | **0.9949** | **0.9690** | **0.9818** | 0.1406 |
| Easy (n=1000) | Fine-tuned | 0.9100 | 0.8480 | 0.8650 | 0.8564 | 0.1665 |

Key takeaways:
- On **hard negatives**, fine-tuning substantially improves ranking accuracy and F1, showing the model learned to separate “twin” proteins from the true match.
- On the **easy sample set**, the baseline is already extremely strong; the hard-negative-only fine-tuning slightly reduces easy-set performance. This suggests some **trade-off** (potentially over-specialization to the mined hard negatives).

### 6.2 Accuracy bar chart (pairwise ranking accuracy)

<img src="evaluation_results/1_accuracy_comparison.png" style="zoom: 25%;" />

Interpretation:
- The finetuned model significantly improves on the **Hard Samples** bar.
- The Easy Samples bar can decrease if fine-tuning focuses only on hard negatives.

## 7. Embedding & Score-space Visual Analysis (Required Screenshots)

### 7.1 Score distribution shift

![](evaluation_results/2_score_distribution.png)

Interpretation:
- **Baseline (left):** positive and negative score distributions overlap heavily (“confused”).
- **Fine-tuned (right):** distributions become more separated, indicating clearer decision boundaries.

### 7.2 t-SNE: embedding space evolution (Top-3 improved cases)

![](evaluation_results/3_tsne_top3_focus.png)

Interpretation:
- Each highlighted case shows one text embedding and its associated anchor/negative protein embeddings.
- The fine-tuned space tends to move the **anchor closer to text** and push the **hard negative farther**.

## 8. Training Analysis: Convergence & Stability

The training loop logs an epoch-level loss history and saves a loss curve.

**Figure (Training loss curve).**

<img src="evaluation_results/loss_curve.png" style="zoom: 67%;" />

Observations (from `weights/ProTrek_35M/loss_history.csv`):
- Loss decreases from **0.1927 → 0.0047** over 30 epochs.
- The curve shows steady convergence with minor fluctuations (e.g., around epoch ~19), consistent with stable fine-tuning.
- No divergence or exploding loss is observed, suggesting the learning rate and margin are reasonable for full fine-tuning.

## 9. Contribution Statement

- Wenyu Li: Construct the hard negative mining pipeline, implement the fine-tuning code, and conduct experiments.
- Zhangchi Huang: Assist in debugging and improving the training and evaluation scripts, and help analyze the results.
- Haotian Shen: Write the report, generate figures, and perform result interpretation and discussion.

---